# train_and_save.py - ä¾›æœ¬åœ°è¨“ç·´ä½¿ç”¨

# =================================================================
# å°å…¥æ‰€æœ‰å¿…è¦çš„åº«
# =================================================================
import requests
import pandas as pd
import datetime
import random
import re
import os
import warnings
import numpy as np
import xgboost as xgb
import json
from datetime import timedelta, timezone
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from meteostat import Point, Hourly, units

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# å‰µå»ºä¸€å€‹ models è³‡æ–™å¤¾ä¾†å„²å­˜æ¨¡å‹
MODELS_DIR = 'models'
os.makedirs(MODELS_DIR, exist_ok=True)

# =================================================================
# è¤‡è£½ app.py ä¸­çš„å¸¸æ•¸è¨­å®š
# =================================================================
API_KEY = "fb579916623e8483cd85344b14605c3109eea922202314c44b87a2df3b1fff77"
API_BASE_URL = "https://api.openaq.org/v3/"
POLLUTANT_TARGETS = ["pm25", "pm10", "o3", "no2", "so2", "co"]
LOCAL_TZ = "Asia/Taipei"
MIN_DATA_THRESHOLD = 100
LAG_HOURS = [1, 2, 3, 6, 12, 24]
ROLLING_WINDOWS = [6, 12, 24]
# ã€ä¿®æ”¹ã€‘è¨­å®šç‚º 90 å¤©ï¼Œæé«˜å°‹æ‰¾æ´»èºæ¸¬ç«™çš„æˆåŠŸç‡
DAYS_TO_FETCH = 90 
N_ESTIMATORS = 150

# =================================================================
# ã€æ ¸å¿ƒä¿®æ”¹ã€‘å…¨çƒè¨“ç·´åœ°é»åˆ—è¡¨ (8 å€‹åœ°é»ï¼Œæ›¿æ›æŸæ—ç‚ºé˜¿å§†æ–¯ç‰¹ä¸¹)
# =================================================================
# train_and_save.py é ‚éƒ¨å¸¸æ•¸éƒ¨åˆ†

# ... (DAYS_TO_FETCH = 90 ä¸è®Š) ...

# ã€æ ¸å¿ƒä¿®æ”¹ã€‘å…¨çƒè¨“ç·´åœ°é»åˆ—è¡¨
# train_and_save.py é ‚éƒ¨å¸¸æ•¸éƒ¨åˆ†

GLOBAL_TRAINING_LOCATIONS = [
    # äºæ´² Aï¼šé«˜æ¿•åº¦/å—äºæ±¡æŸ“ (é«˜æ•¸æ“šé‡)
    (22.6273, 120.3014, "Kaohsiung, TW"),     
    (28.7041, 77.1025, "Delhi, IN"),         
    
    # åŒ—ç¾æ´² (é«˜ç›£æ¸¬æ¨™æº–)
    (40.7128, -74.0060, "New York, US"),       
    (43.6532, -79.3832, "Toronto, CA"),        
    
    # æ­æ´² (æ•¸æ“šç©©å®š)
    (52.3676, 4.9041, "Amsterdam, NL"),       
    (48.8566, 2.3522, "Paris, FR"),            
    
    # äºæ´² Bï¼š**æ¥µè‡´æ•¸æ“šå®Œæ•´é»** (æ›¿æ›å¤±æ•—é»ï¼Œç¢ºä¿æˆåŠŸ)
    (39.9042, 116.4074, "Beijing, CN"),        # ä¸­åœ‹åŒ—äº¬
    
    # äºæ´² Cï¼šæ±äºäº¤é€š/å·¥æ¥­ (æ•¸æ“šç©©å®š)
    (35.6895, 139.6917, "Tokyo, JP"),          # æ—¥æœ¬æ±äº¬
]


# ç°¡åŒ–çš„ AQI åˆ†ç´šè¡¨ (ä¿æŒä¸è®Š)
AQI_BREAKPOINTS = {
    "pm25": [(0.0, 12.0, 0, 50), (12.1, 35.4, 51, 100), (35.5, 55.4, 101, 150), (55.5, 150.4, 151, 200)],
    "pm10": [(0, 54, 0, 50), (55, 154, 51, 100), (155, 254, 101, 150), (255, 354, 151, 200)],
    "o3": [(0, 54, 0, 50), (55, 70, 51, 100), (71, 85, 101, 150), (86, 105, 151, 200)],
    "co": [(0.0, 4.4, 0, 50), (4.5, 9.4, 51, 100), (9.5, 12.4, 101, 150), (12.5, 15.4, 151, 200)],
    "no2": [(0, 100, 0, 50), (101, 360, 51, 100), (361, 649, 101, 150), (650, 1249, 151, 200)],
    "so2": [(0, 35, 0, 50), (36, 75, 51, 100), (76, 185, 101, 150), (186, 304, 151, 200)],
}

# =================================================================
# AQI è¼”åŠ©å‡½å¼ (ä¿æŒä¸è®Š)
# =================================================================
def calculate_aqi_sub_index(param: str, concentration: float) -> float:
    """è¨ˆç®—å–®ä¸€æ±¡æŸ“ç‰©æ¿ƒåº¦å°æ‡‰çš„ AQI å­æŒ‡æ•¸ (I)"""
    if pd.isna(concentration) or concentration < 0:
        return 0

    breakpoints = AQI_BREAKPOINTS.get(param)
    if not breakpoints:
        return 0

    for C_low, C_high, I_low, I_high in breakpoints:
        if C_low <= concentration <= C_high:
            if C_high == C_low:
                return I_high
            I = ((I_high - I_low) / (C_high - C_low)) * (concentration - C_low) + I_low
            return np.round(I)

        if concentration > breakpoints[-1][1]:
            I_low, I_high = breakpoints[-1][2], breakpoints[-1][3]
            C_low, C_high = breakpoints[-1][0], breakpoints[-1][1]
            if C_high == C_low:
                return I_high
            I_rate = (I_high - I_low) / (C_high - C_low)
            I = I_high + I_rate * (concentration - C_high)
            return np.round(I)

    return 0

def calculate_aqi(row: pd.Series, params: list) -> int:
    """æ ¹æ“šå¤šå€‹æ±¡æŸ“ç‰©æ¿ƒåº¦è¨ˆç®—æœ€çµ‚ AQI (å–æœ€å¤§å­æŒ‡æ•¸)"""
    sub_indices = []
    for p in params:
        col_name = f'{p}_pred' if f'{p}_pred' in row else f'{p}_value'
        if col_name in row and not pd.isna(row[col_name]):
            sub_index = calculate_aqi_sub_index(p, row[col_name])
            sub_indices.append(sub_index)

    if not sub_indices:
        return np.nan

    return int(np.max(sub_indices))


# =================================================================
# OpenAQ è¼”åŠ©å‡½å¼ (ä¿æŒä¸è®Š)
# =================================================================
def sanitize_filename(name: str) -> str:
    return re.sub(r'[\\/:"*?<>|]+', '_', name)

def get_nearest_station(lat, lon, radius=20000, limit=50, days=DAYS_TO_FETCH):
    """ æ‰¾é›¢ (lat,lon) æœ€è¿‘ä¸”æœ€è¿‘ days å…§æœ‰æ›´æ–°çš„æ¸¬ç«™ """
    url = f"{API_BASE_URL}locations"
    headers = {"X-API-Key": API_KEY}
    params = {"coordinates": f"{lat},{lon}", "radius": radius, "limit": limit}
    try:
        resp = requests.get(url, headers=headers, params=params)
        resp.raise_for_status()
        j = resp.json()
    except Exception as e:
        return None

    if "results" not in j or not j["results"]:
        return None

    df = pd.json_normalize(j["results"])
    if "datetimeLast.utc" not in df.columns:
        return None

    df["datetimeLast.utc"] = pd.to_datetime(df["datetimeLast.utc"], errors="coerce", utc=True)
    now = pd.Timestamp.utcnow()
    cutoff = now - pd.Timedelta(days=days) 
    # é€™è£¡ä½¿ç”¨ DAYS_TO_FETCH=90 å¤©ä¾†ç¯©é¸æ´»èºæ¸¬ç«™
    df = df[(df["datetimeLast.utc"] >= cutoff) & (df["datetimeLast.utc"] <= now)] 
    if df.empty:
        return None

    nearest = df.sort_values("distance").iloc[0]
    return nearest.to_dict()

def get_station_sensors(station_id):
    """ ä½¿ç”¨ /locations/{id}/sensors å–å¾— sensors åˆ—è¡¨ """
    url = f"{API_BASE_URL}locations/{station_id}/sensors"
    headers = {"X-API-Key": API_KEY}
    try:
        resp = requests.get(url, headers=headers, params={"limit":1000})
        resp.raise_for_status()
        j = resp.json()
        return j.get("results", [])
    except Exception as e:
        return []

def _extract_datetime_from_measurement(item: dict):
    """ å˜—è©¦å¾ measurement ç‰©ä»¶æŠ½å‡ºæ™‚é–“å­—ä¸² """
    candidates = [("period", "datetimeFrom", "utc"), ("date", "utc"), ("datetime",)]
    for path in candidates:
        cur = item
        ok = True
        for k in path:
            if isinstance(cur, dict) and k in cur:
                cur = cur[k]
            else:
                ok = False
                break
        if ok and cur:
            return cur
    return None

def fetch_sensor_data(sensor_id, param_name, limit=500, days=DAYS_TO_FETCH):
    """ æ“·å– sensor çš„æ™‚é–“åºåˆ— """
    url = f"{API_BASE_URL}sensors/{sensor_id}/measurements"
    headers = {"X-API-Key": API_KEY}
    now = datetime.datetime.now(datetime.timezone.utc)
    date_from = (now - datetime.timedelta(days=days)).isoformat().replace("+00:00", "Z")
    # é€™è£¡çš„ limit=500 æ˜¯ OpenAQ API çš„ç¡¬é™åˆ¶
    params = {"limit": limit, "date_from": date_from} 

    try:
        resp = requests.get(url, headers=headers, params=params)
        resp.raise_for_status()
        j = resp.json()
        results = j.get("results", [])
    except Exception as e:
        return pd.DataFrame()

    rows = []
    for r in results:
        dt_str = _extract_datetime_from_measurement(r)
        try:
            ts = pd.to_datetime(dt_str, utc=True)
        except Exception:
            ts = pd.NaT
        rows.append({"datetime": ts, param_name: r.get("value")})

    df = pd.DataFrame(rows).dropna(subset=["datetime"])
    if df.empty:
        return pd.DataFrame()
    df = df.sort_values("datetime", ascending=False).drop_duplicates(subset=["datetime"])
    return df

def get_all_target_data(station_id, target_params, days_to_fetch):
    """ç²å–æ‰€æœ‰ç›®æ¨™æ±¡æŸ“ç‰©æ•¸æ“šä¸¦åˆä½µ"""
    sensors = get_station_sensors(station_id)
    sensor_map = {s.get("parameter", {}).get("name", "").lower(): s.get("id") for s in sensors}

    all_dfs = []
    found_params = []

    for param in target_params:
        sensor_id = sensor_map.get(param)
        if sensor_id:
            df_param = fetch_sensor_data(sensor_id, param, limit=500, days=days_to_fetch)
            if not df_param.empty:
                df_param.rename(columns={param: f'{param}_value'}, inplace=True)
                all_dfs.append(df_param)
                found_params.append(param)

    if not all_dfs:
        return pd.DataFrame(), []

    merged_df = all_dfs[0]
    for i in range(1, len(all_dfs)):
        merged_df = pd.merge(merged_df, all_dfs[i], on='datetime', how='outer')

    return merged_df, found_params

# =================================================================
# WeatherCrawler é¡ (ä¿æŒä¸è®Š)
# =================================================================
class WeatherCrawler:
    """Meteostat å°æ™‚ç´šå¤©æ°£æ•¸æ“šçˆ¬èŸ²èˆ‡æ•´åˆ"""

    def __init__(self, lat, lon):
        self.point = Point(lat, lon)
        self.weather_cols = {
            'temp': 'temperature',
            'rhum': 'humidity',
            'pres': 'pressure',
        }

    def fetch_and_merge_weather(self, air_quality_df: pd.DataFrame):
        """æ ¹æ“šç©ºæ°£å“è³ªæ•¸æ“šçš„æ™‚é–“ç¯„åœï¼Œå¾ Meteostat ç²å–å°æ™‚ç´šå¤©æ°£æ•¸æ“šä¸¦åˆä½µã€‚"""
        if air_quality_df.empty:
            return air_quality_df

        if air_quality_df['datetime'].dt.tz is None:
            air_quality_df['datetime'] = air_quality_df['datetime'].dt.tz_localize('UTC')

        start_time_utc_aware = air_quality_df['datetime'].min()
        end_time_utc_aware = air_quality_df['datetime'].max()

        start_dt = start_time_utc_aware.tz_convert(None).to_pydatetime()
        end_dt = end_time_utc_aware.tz_convert(None).to_pydatetime()

        try:
            data = Hourly(self.point, start_dt, end_dt)
            weather_data = data.fetch()
        except Exception as e:
            weather_data = pd.DataFrame()

        if weather_data.empty:
            empty_weather = pd.DataFrame({'datetime': air_quality_df['datetime'].unique()})
            for col in self.weather_cols.values():
                empty_weather[col] = np.nan
            return pd.merge(air_quality_df, empty_weather, on='datetime', how='left')

        weather_data = weather_data.reset_index()
        weather_data.rename(columns={'time': 'datetime'}, inplace=True)
        weather_data = weather_data.rename(columns=self.weather_cols)
        weather_data = weather_data[list(self.weather_cols.values()) + ['datetime']]
        weather_data['datetime'] = weather_data['datetime'].dt.tz_localize('UTC')

        merged_df = pd.merge(
            air_quality_df,
            weather_data,
            on='datetime',
            how='left'
        )

        weather_cols_list = list(self.weather_cols.values())
        merged_df[weather_cols_list] = merged_df[weather_cols_list].fillna(method='ffill').fillna(method='bfill')

        return merged_df

    def get_weather_feature_names(self):
        return list(self.weather_cols.values())


# =================================================================
# ç‰¹å¾µå·¥ç¨‹è¼”åŠ©å‡½å¼ (ä¿æŒä¸è®Š)
# =================================================================
def _preprocess_and_feature_engineer(df_input: pd.DataFrame, pollutant_params: list, weather_feature_names: list) -> pd.DataFrame:
    """è™•ç†å–®ä¸€åœ°é»çš„æ•¸æ“šã€é‡æ¡æ¨£ã€è¨ˆç®— AQI å’Œæ‰€æœ‰ç‰¹å¾µã€‚"""
    
    df = df_input.copy()
    value_cols = [f'{p}_value' for p in pollutant_params]
    all_data_cols = value_cols + weather_feature_names

    # é‡æ¡æ¨£åˆ°å°æ™‚
    df.set_index('datetime', inplace=True)
    df = df[value_cols + weather_feature_names].resample('H').mean()
    df.reset_index(inplace=True)
    df = df.dropna(how='all', subset=all_data_cols)

    # è¨ˆç®—æ­·å² AQI
    df['aqi_value'] = df.apply(lambda row: calculate_aqi(row, pollutant_params), axis=1)

    # ç§»é™¤ä»»ä¸€æ±¡æŸ“ç‰©æˆ–å¤©æ°£æ•¸æ“šç‚º NaN çš„è¡Œ (ç¢ºä¿æ¨¡å‹è¼¸å…¥å®Œæ•´)
    df = df.dropna(subset=all_data_cols + ['aqi_value']).reset_index(drop=True)
    
    if len(df) <= max(LAG_HOURS):
        return pd.DataFrame()

    # ç‰¹å¾µå·¥ç¨‹
    df['hour'] = df['datetime'].dt.hour
    df['day_of_week'] = df['datetime'].dt.dayofweek
    df['month'] = df['datetime'].dt.month
    df['day_of_year'] = df.index # ä½¿ç”¨ç°¡å–®çš„è¡Œç´¢å¼•ä½œç‚ºå¹´å…§å¤©æ•¸çš„æ›¿ä»£
    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
    # ç”±æ–¼æˆ‘å€‘æ²’æœ‰ 365 å¤©æ•¸æ“šï¼Œé€™è£¡çš„ Day_sin/cos åªèƒ½åŸºæ–¼ç›¸å°ç´¢å¼•ï¼Œä½†ä»ä¿ç•™
    df['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365) 
    df['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)

    df = df.sort_values('datetime')
    feature_base_cols = value_cols + ['aqi_value']

    for col_name in feature_base_cols:
        param = col_name.replace('_value', '')
        for lag in LAG_HOURS:
            df[f'{param}_lag_{lag}h'] = df[col_name].shift(lag)

        if 'aqi' not in param:
            for window in ROLLING_WINDOWS:
                df[f'{param}_rolling_mean_{window}h'] = df[col_name].rolling(window=window, min_periods=1).mean()
                df[f'{param}_rolling_std_{window}h'] = df[col_name].rolling(window=window, min_periods=1).std()

    # ç§»é™¤å› ç‚º lag/rolling å‰µå»ºçš„ NaN è¡Œ
    df = df.dropna().reset_index(drop=True)
    
    return df


# =================================================================
# è¨“ç·´èˆ‡å„²å­˜æ¨¡å‹çš„é‚è¼¯ (ä¿æŒä¸è®Š)
# =================================================================
def train_and_save_models(locations: list, days_to_fetch: int):
    print(f"ğŸ”¥ [Local Init] é–‹å§‹åŸ·è¡Œå¤šåœ°é» AQI é æ¸¬åˆå§‹åŒ–æµç¨‹ (å°‹æ‰¾æ´»èºæ¸¬ç«™ç¯„åœ: {days_to_fetch} å¤©)...")

    # 1. å¤šåœ°é»æ•¸æ“šæ”¶é›†ã€è™•ç†èˆ‡åˆä½µ
    all_df = []
    all_found_params = set()
    weather_feature_names = WeatherCrawler(0, 0).get_weather_feature_names() 

    for lat, lon, name in locations:
        print(f"\n--- ğŸŒ è™•ç†åœ°é»: {name} ({lat:.4f}, {lon:.4f}) ---")
        
        weather = WeatherCrawler(lat, lon)
        
        try:
            # é€™è£¡ä½¿ç”¨ DAYS_TO_FETCH=90 å°‹æ‰¾æœ€è¿‘ 90 å¤©æœ‰æ•¸æ“šçš„æ¸¬ç«™
            station = get_nearest_station(lat, lon, days=days_to_fetch) 
            
            if not station:
                print(f"ğŸš¨ [Init - {name}] æœªæ‰¾åˆ°æ´»èºæ¸¬ç«™ï¼Œè·³éæ­¤åœ°é»ã€‚")
                continue
            
            print(f"âœ… [Init - {name}] æ‰¾åˆ°æ¸¬ç«™: {station['name']} ({station['id']})")
            # å¯¦éš›æŠ“å–æ•¸æ“šï¼ˆä»å— OpenAQ çš„ 500 ç­†é™åˆ¶ï¼‰
            df_raw, found_params = get_all_target_data(station["id"], POLLUTANT_TARGETS, days_to_fetch)
            
            print(f"   [Init - {name}] åŸå§‹æ•¸æ“šé»æ•¸: {len(df_raw)}")
            if df_raw.empty or len(df_raw) < MIN_DATA_THRESHOLD:
                print(f"ğŸš¨ [Init - {name}] åŸå§‹æ•¸æ“šé‡ä¸è¶³ ({len(df_raw)}), è·³éæ­¤åœ°é»ã€‚")
                continue
                
            # åˆä½µ Meteostat å¤©æ°£æ•¸æ“š
            df = weather.fetch_and_merge_weather(df_raw.copy())
            
            # æ•¸æ“šæ¸…ç†èˆ‡ç‰¹å¾µå·¥ç¨‹
            df_processed = _preprocess_and_feature_engineer(df, found_params, weather_feature_names)
            
            if not df_processed.empty:
                all_df.append(df_processed)
                all_found_params.update(found_params)
                print(f"ğŸ“Š [Init - {name}] **æœ€çµ‚è¨“ç·´æ•¸æ“šé‡**: {len(df_processed)} å°æ™‚")
            else:
                print(f"ğŸš¨ [Init - {name}] ç‰¹å¾µå·¥ç¨‹å¾Œæ•¸æ“šé‡ä¸è¶³ï¼ˆå°æ–¼ {max(LAG_HOURS)}ï¼‰ï¼Œè·³éæ­¤åœ°é»ã€‚")

        except Exception as e:
            print(f"âŒ [Init - {name}] è™•ç†å¤±æ•—: {e}")
            continue

    # 2. åˆä½µæ‰€æœ‰æ•¸æ“šä¸¦æº–å‚™è¨“ç·´
    if not all_df:
        raise ValueError("æ‰€æœ‰åœ°é»æ•¸æ“šæ”¶é›†å¤±æ•—ï¼Œè¨“ç·´ç„¡æ³•é€²è¡Œã€‚")
        
    final_df = pd.concat(all_df, ignore_index=True)
    final_df = final_df.sort_values('datetime').reset_index(drop=True)
    
    POLLUTANT_PARAMS_TRAINED = list(POLLUTANT_TARGETS)
    
    print(f"\n=========================================================")
    print(f"ğŸ“Š [Local Init] æœ€çµ‚ç”¨æ–¼è¨“ç·´çš„**ç¸½æ•¸æ“šé‡**: {len(final_df)} å°æ™‚")
    print(f"ğŸ¯ [Local Init] è¨“ç·´ç›®æ¨™æ±¡æŸ“ç‰©: {POLLUTANT_PARAMS_TRAINED}")
    print(f"=========================================================")

    if len(final_df) == 0:
        raise ValueError("ç¸½æ•¸æ“šé‡ç‚ºé›¶ï¼Œè¨“ç·´ç„¡æ³•é€²è¡Œã€‚")

    LAST_OBSERVATION = final_df.iloc[-1:].to_json(orient='records', date_format='iso')

    # 3. ç¢ºå®šç‰¹å¾µæ¬„ä½
    base_time_features = ['hour', 'day_of_week', 'month', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos']
    
    air_quality_features = []
    for param in POLLUTANT_TARGETS + ['aqi']: 
        for lag in LAG_HOURS:
            air_quality_features.append(f'{param}_lag_{lag}h')
        if param != 'aqi':
            for window in ROLLING_WINDOWS:
                air_quality_features.append(f'{param}_rolling_mean_{window}h')
                air_quality_features.append(f'{param}_rolling_std_{window}h')

    FEATURE_COLUMNS = weather_feature_names + base_time_features + air_quality_features
    FEATURE_COLUMNS = [col for col in FEATURE_COLUMNS if col in final_df.columns]

    # 4. æ•¸æ“šåˆ†å‰²èˆ‡æ¨¡å‹è¨“ç·´
    # train_and_save.py (ä¿®æ”¹ train_and_save_models å‡½å¼æœ«æ®µ)

    # 4. æ•¸æ“šåˆ†å‰²èˆ‡æ¨¡å‹è¨“ç·´ (80% è¨“ç·´)
    split_idx = int(len(final_df) * 0.8)

    # ã€æ ¸å¿ƒä¿®æ”¹é» A: æ¸…ç† NaN ç›®æ¨™ã€‘
    # åœ¨æå– X å’Œ Y ä¹‹å‰ï¼Œå…ˆç§»é™¤æ‰€æœ‰è¨“ç·´ç›®æ¨™ï¼ˆYï¼‰æ¬„ä½ä¸­å«æœ‰ NaN çš„è¡Œã€‚
    # é›–ç„¶ä¹‹å‰å·²ç¶“åšéä¸€æ¬¡ dropnaï¼Œé€™è£¡çš„é¡å¤–æª¢æŸ¥æ˜¯ç‚ºäº†è™•ç†åˆä½µå¾Œçš„æ¥µç«¯æƒ…æ³ã€‚
    Y_cols = [f'{p}_value' for p in POLLUTANT_TARGETS]
    final_df.dropna(subset=Y_cols, inplace=True) # ç§»é™¤è¨“ç·´ç›®æ¨™ï¼ˆYï¼‰æ¬„ä½æœ‰ NaN çš„è¡Œ

    # é‡æ–°ç¢ºä¿æ•¸æ“šé‡ä»è¶³å¤ 
    if len(final_df) == 0:
        raise ValueError("æœ€çµ‚æ•¸æ“šæ¸…ç†å¾Œç¸½æ•¸æ“šé‡ç‚ºé›¶ï¼Œè¨“ç·´ç„¡æ³•é€²è¡Œã€‚")
        
    # é‡æ–°è¨ˆç®—åˆ†å‰²ç´¢å¼•
    split_idx = int(len(final_df) * 0.8) 

    X = final_df[FEATURE_COLUMNS]
    # é€™è£¡ Y çš„å­—å…¸å‰µå»ºæ‡‰è©²åªåŒ…å« final_df ä¸­å­˜åœ¨çš„æ¬„ä½
    Y = {param: final_df[f'{param}_value'] 
        for param in POLLUTANT_TARGETS 
        if f'{param}_value' in final_df.columns} 

    X_train = X[:split_idx]
    # ... (å…¶é¤˜è¨“ç·´ä»£ç¢¼ä¸è®Š)
    
    print(f"â³ [Local Init] é–‹å§‹è¨“ç·´ {len(Y)} å€‹ XGBoost æ¨¡å‹ (N={N_ESTIMATORS})...")
    TRAINED_MODELS = {}
    
    for param, Y_series in Y.items():
        Y_train = Y_series[:split_idx]
        print(f" Â  Â  Â  è¨“ç·´ {param} æ¨¡å‹...")
        
        xgb_model = xgb.XGBRegressor(
            n_estimators=N_ESTIMATORS, max_depth=7, learning_rate=0.08, random_state=42, n_jobs=-1
        )
        xgb_model.fit(X_train, Y_train)
        TRAINED_MODELS[param] = xgb_model
        
        model_path = os.path.join(MODELS_DIR, f'{param}_model.json')
        xgb_model.save_model(model_path)
        print(f" Â  Â  Â  âœ… {param} æ¨¡å‹å·²å„²å­˜è‡³ {model_path}")

    # 5. å„²å­˜æ¨¡å‹å…ƒæ•¸æ“š (Metadata)
    metadata = {
        'pollutant_params': POLLUTANT_TARGETS, 
        'feature_columns': FEATURE_COLUMNS,
        'last_observation_json': LAST_OBSERVATION
    }
    with open(os.path.join(MODELS_DIR, 'model_meta.json'), 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=4)

    print("âœ… [Local Init] æ‰€æœ‰æ¨¡å‹å’Œå…ƒæ•¸æ“šå„²å­˜å®Œæˆã€‚")

if __name__ == '__main__':
    try:
        train_and_save_models(GLOBAL_TRAINING_LOCATIONS, DAYS_TO_FETCH)
    except Exception as e:
        print(f"âŒ [Local Init] è¨“ç·´åŸ·è¡Œå¤±æ•—: {e}")